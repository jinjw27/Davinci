import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import onnx
import os
import random

# ==========================================
# 1. ëª¨ë¸ ì•„í‚¤í…ì²˜ (52 -> 128 -> 128 -> 64 -> 24)
# ==========================================
class DaVinciRLModel(nn.Module):
    def __init__(self):
        super(DaVinciRLModel, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(52, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 24)
        )

    def forward(self, x):
        return self.network(x)

# ==========================================
# 2. ê°•í™”í•™ìŠµ í™˜ê²½ (Environment)
# ==========================================
class DaVinciEnv:
    def __init__(self):
        self.reset()

    def reset(self):
        # 0-11 Black (0-11), 0-11 White (12-23)
        self.deck = list(range(24))
        random.shuffle(self.deck)
        
        # 4ê°œì”© ë¶„ë°°
        self.ai_hand = sorted(self.deck[:4])
        self.opp_hand = sorted(self.deck[4:8])
        self.deck = self.deck[8:]
        
        self.ai_revealed = [False] * 24
        self.opp_revealed = [False] * 24
        self.turn_count = 0
        return self.get_state()

    def get_state(self):
        state = np.zeros(52, dtype=np.float32)
        # ë‚´ íƒ€ì¼ [0-23]
        for idx in self.ai_hand:
            state[idx] = 1.0
        # ìƒëŒ€ ê³µê°œ íƒ€ì¼ [24-47]
        for i in range(24):
            if self.opp_revealed[i]:
                state[24 + i] = 1.0
        # ì •ë³´ [48-51]
        state[48] = self.turn_count / 20.0
        state[49] = len(self.ai_hand) / 12.0
        state[50] = len(self.opp_hand) / 12.0
        state[51] = len(self.deck) / 24.0
        return state

    def step(self, action_idx):
        # action_idx: 0-23 (ì¶”ì¸¡í•  ì¹´ë“œ ë²ˆí˜¸)
        reward = 0
        done = False
        
        # ìœ íš¨í•œ ì¶”ì¸¡ì¸ì§€ í™•ì¸ (ìƒëŒ€ íŒ¨ì— ìˆê³  ì•„ì§ ì•ˆ ì•Œë ¤ì§„ ì¹´ë“œ)
        if action_idx in self.opp_hand and not self.opp_revealed[action_idx]:
            self.opp_revealed[action_idx] = True
            reward = 1.0 # ì„±ê³µ ë³´ìƒ
            if all(self.opp_revealed[h] for h in self.opp_hand):
                reward += 5.0 # ì¥ê¸°ì  ë³´ìƒ (ìŠ¹ë¦¬)
                done = True
        else:
            reward = -1.0 # ì‹¤íŒ¨ íŒ¨ë„í‹°
            # ì‹¤íŒ¨ ì‹œ ë‚´ ì¹´ë“œ ì¤‘ í•˜ë‚˜ ê³µê°œ (í¸ì˜ìƒ ì²« ë²ˆì§¸ ë¹„ê³µê°œ ì¹´ë“œ)
            for h in self.ai_hand:
                if not self.ai_revealed[h]:
                    self.ai_revealed[h] = True
                    break
            if all(self.ai_revealed[h] for h in self.ai_hand):
                reward -= 2.0 # íŒ¨ë°° íŒ¨ë„í‹°
                done = True
        
        self.turn_count += 1
        if self.turn_count >= 30: done = True
        
        return self.get_state(), reward, done

# ==========================================
# 3. ê°•í™”í•™ìŠµ (Policy Gradient ê¸°ë°˜ ë‹¨ìˆœí™”)
# ==========================================
def train_rl(episodes=2000):
    env = DaVinciEnv()
    model = DaVinciRLModel()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    
    print(f"ê°•í™”í•™ìŠµ ì‹œì‘: {episodes} ì—í”¼ì†Œë“œ...")
    
    for ep in range(episodes):
        state = env.reset()
        log_probs = []
        rewards = []
        
        done = False
        while not done:
            state_t = torch.FloatTensor(state).unsqueeze(0)
            probs = torch.softmax(model(state_t), dim=1)
            
            # Action sampling
            m = torch.distributions.Categorical(probs)
            action = m.sample()
            log_prob = m.log_prob(action)
            
            next_state, reward, done = env.step(action.item())
            
            log_probs.append(log_prob)
            rewards.append(reward)
            state = next_state

        # Update policy
        R = 0
        policy_loss = []
        returns = []
        for r in rewards[::-1]:
            R = r + 0.9 * R
            returns.insert(0, R)
        
        returns = torch.tensor(returns)
        if len(returns) > 1:
            returns = (returns - returns.mean()) / (returns.std() + 1e-6)
        
        for log_prob, Gt in zip(log_probs, returns):
            policy_loss.append(-log_prob * Gt)
        
        optimizer.zero_grad()
        policy_loss = torch.stack(policy_loss).sum()
        policy_loss.backward()
        optimizer.step()
        
        if (ep + 1) % 500 == 0:
            print(f"   Episode {ep+1}: Last Total Reward = {sum(rewards):.2f}")

    print("ê°•í™”í•™ìŠµ ì™„ë£Œ!")
    return model

# ==========================================
# 4. ì €ì¥ ë° ê²€ì¦
# ==========================================
def main():
    model = train_rl(3000)
    model.eval()
    
    # íŒŒë¼ë¯¸í„° í™•ì¸
    total_params = sum(p.numel() for p in model.parameters())
    weight_sum = sum(p.sum().item() for p in model.parameters())
    print(f"--- ëª¨ë¸ ìƒíƒœ í™•ì¸ ---")
    print(f"ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params}")
    print(f"ê°€ì¤‘ì¹˜ í•©ê³„: {weight_sum:.4f}")
    
    output_path = "davinci_ai.onnx"
    dummy_input = torch.randn(1, 52)
    
    # [ë³€ê²½] Opset 12ë¡œ í•˜í–¥ ì¡°ì • (í˜¸í™˜ì„± ê·¹ëŒ€í™”)
    print(f"4. ONNX ë‚´ë³´ë‚´ê¸° ì¤‘ (Opset 12)...")
    torch.onnx.export(
        model, dummy_input, output_path,
        export_params=True, 
        opset_version=12,
        do_constant_folding=True,
        input_names=['input'], output_names=['output'],
        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}
    )

    import onnx
    import os
    if not os.path.exists(output_path):
        print("âŒ ëª¨ë¸ íŒŒì¼ ìƒì„± ì‹¤íŒ¨!")
        return

    file_size = os.path.getsize(output_path) / 1024
    print(f"âœ… ìƒì„±ëœ íŒŒì¼ í¬ê¸°: {file_size:.2f} KB")

    try:
        onnx_model = onnx.load(output_path)
        # ë§Œì•½ IR ë²„ì „ì´ ë„ˆë¬´ ë†’ìœ¼ë©´ ê°•ì œë¡œ ë‚®ì¶¤ (ORT-Web í˜¸í™˜ì„±)
        if onnx_model.ir_version > 8:
            print(f"âš ï¸ IR ë²„ì „ì´ ë„ˆë¬´ ë†’ìŒ ({onnx_model.ir_version}) -> 8ë¡œ ì¡°ì •")
            onnx_model.ir_version = 8
            onnx.save(onnx_model, output_path)

        onnx.checker.check_model(onnx_model)
        print(f"âœ… ëª¨ë¸ ê²€ì¦ ì™„ë£Œ: {output_path}")
        print(f"âœ… ìµœì¢… IR Version: {onnx_model.ir_version}")
        print(f"âœ… ìµœì¢… Opset Version: {onnx_model.opset_import[0].version}")
    except Exception as e:
        print(f"âŒ ê²€ì¦ ì‹¤íŒ¨: {e}")

    print("\nğŸ‰ ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()
